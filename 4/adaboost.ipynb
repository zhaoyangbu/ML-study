{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error by using sklearn adaboost:  0.17725752508361203\n",
      "Testing error by using sklearn adaboost:  0.29333333333333333\n",
      "Training error by using self adaboost:  0.22742474916387959\n",
      "Testing error by using self adaboost:  0.32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "filename1 = 'blood_train.csv'\n",
    "filename2 = 'blood_test.csv'\n",
    "data1 = np.loadtxt(filename1, delimiter=',')\n",
    "data2 = np.loadtxt(filename2, delimiter=',')\n",
    "x_train = data1[:,0:-1]\n",
    "y_train = data1[:,-1]\n",
    "x_test = data2[:,0:-1]\n",
    "y_test = data2[:,-1]\n",
    "\n",
    "#Implement sklearn \n",
    "sklearn_imp = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=10)\n",
    "sklearn_imp.fit(x_train,y_train)\n",
    "\n",
    "y_train_pre = sklearn_imp.predict(x_train)\n",
    "sk_trerror = sum(y_train_pre!= y_train) / len(y_train)\n",
    "print('Training error by using sklearn adaboost: ', sk_trerror)\n",
    "\n",
    "y_test_pre = sklearn_imp.predict(x_test)\n",
    "sk_tserror = sum(y_test_pre!= y_test) / len(y_test)\n",
    "print('Testing error by using sklearn adaboost: ', sk_tserror)\n",
    "\n",
    "#Self adaboost algorithm\n",
    "def sample(x, y, D, n):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(n):\n",
    "        num = int(np.ceil(D[i]*n)) \n",
    "        for j in range(num):\n",
    "            X.append(x[i,:])\n",
    "            Y.append(y[i])\n",
    "    return np.matrix(X)[0:n,:], np.array(Y)[0:n]\n",
    "\n",
    "def classify(data):\n",
    "    n=len(data)\n",
    "    label=[]\n",
    "    for i in range (n):\n",
    "        if(data[i]>0.0):\n",
    "            label.append(1.0)\n",
    "        else:\n",
    "            label.append(0.0)\n",
    "    return label\n",
    "\n",
    "def prob_distribution(et, at, y, htx, D):\n",
    "    zt = 2.0* math.sqrt(et*(1-et))#normalization constant\n",
    "    size_d = len(D)\n",
    "    new_d = np.zeros(size_d)\n",
    "    for i in range (size_d):\n",
    "        new_d[i] = ((D[i]/zt) * math.exp((-at)* y[i] * htx[i])) / np.sum(new_d)\n",
    "    return new_d\n",
    "\n",
    "class adaboost():\n",
    "    def __init__(self, rounds=20, weak_clf = []):\n",
    "        self.rounds=rounds\n",
    "        self.H = []\n",
    "        self.at = []\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        row, col = np.shape(x)\n",
    "        D = (np.ones((row))) / row\n",
    "        np.shape(D)\n",
    "        weak_clf = DecisionTreeClassifier(max_depth=2)\n",
    "        y = np.transpose(y)\n",
    "\n",
    "        for t in range (self.rounds): \n",
    "            xs, ys = sample(x_train, y_train, D, row)\n",
    "            weak_clf.fit(xs, ys)\n",
    "            htx = weak_clf.predict(xs)\n",
    "            error_h = np.sum(np.abs(np.subtract(y_train,htx))) / len(y_train)\n",
    "            size_d = len(D)\n",
    "            mistake = np.abs(np.subtract(y, htx))\n",
    "            error = 0.0\n",
    "        \n",
    "            for i in range (size_d):\n",
    "                total_error = error + (D[i]*mistake[i])\n",
    "                alpha_t = 0.5 * math.log10((1-total_error)/ total_error)\n",
    "                self.at.append(alpha_t)\n",
    "                new_prob_distribution = prob_distribution(total_error, alpha_t, y, htx, D)\n",
    "                self.H.append(weak_clf)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        row, col = np.shape(x)\n",
    "        y_pred = np.zeros(row)\n",
    "        for clf, a in zip(self.H,self.at):\n",
    "            y = clf.predict(x)\n",
    "            y_pred = y_pred + (a * y)\n",
    "        y_pred = classify(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "adaboost = adaboost(rounds=20)\n",
    "adaboost.fit(x_train, y_train)\n",
    "y_train_pred = adb.predict(x_train)    \n",
    "y_test_pred = adb.predict(x_test)\n",
    "training_error = sum(y_train_pred != y_train) / len(y_train)\n",
    "testing_error = sum(y_test_pred != y_test) / len(y_test)\n",
    "\n",
    "print('Training error by using self adaboost: ', training_error)\n",
    "print('Testing error by using self adaboost: ', testing_error)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
